#!/usr/bin/env python
"""Convert a point source discharge spreadsheet into SSM input files

Script that converts a point-source spreadsheet in the format generated by
ssm_read_fwinputs.py into a pair of input files, one for FVCOM and one for
FVCOM-ICM.
"""

from argparse import ArgumentParser, FileType
import logging
import sys

import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)

# All state variables that go in the WQ input file, in order
ALL_STATEVARS = ('discharge', 'temp', 'salt', 'tss',  'alg1', 'alg2', 'alg3',
                 'zoo1', 'zoo2', 'ldoc', 'rdoc', 'lpoc', 'rpoc', 'nh4', 
                 'no32', 'urea', 'ldon', 'rdon', 'lpon', 'rpon', 'po4',
                 'ldop', 'rdop', 'lpop', 'rpop', 'pip', 'cod', 'doxg', 'psi',
                 'dsi', 'alg1p','alg2p','alg3p', 'dic', 'talk')

def read_data(f):
    """Read an excel spreadsheet into DataFrames"""
    dfs = {
        'nodes': pd.read_excel(f, sheet_name='Nodes', index_col='Node'),
        'vqdist': pd.read_excel(f, sheet_name='VQDist', index_col='Node'),
        'data': pd.read_excel(f, sheet_name='Data', index_col=[0,1])
    }
    return dfs

def check_data(dfs, start_date):
    """Sanity checks on the spreadsheet to make sure it was edited correctly"""

    # Check that every node listed in nodes has entries in vqdist and data
    if dfs['nodes'].index.has_duplicates:
        return (False, 'Duplicate value(s) in Nodes')
    if dfs['vqdist'].index.has_duplicates:
        return (False, 'Duplicate value(s) in VQDist')
    if dfs['data'].index.has_duplicates:
        return (False, 'Duplicate value(s) in Data')
    if np.any(~np.isin(dfs['vqdist'].index.values, dfs['nodes'].index.values, assume_unique=True)):
        return (False, 'At least one node in VQDist missing from Nodes')
    if np.any(~np.isin(dfs['data'].index.get_level_values(1),
        dfs['nodes'].index.values, assume_unique=True)):
        return (False, 'At least one node in Data missing from Nodes')

    # Check dtypes in data and vqdist
    if np.any((dfs['data'].dtypes != np.float64) & (dfs['data'].dtypes != np.int64)):
        return (False, 'At least one wrong dtype in the Data columns')
    if np.any(dfs['vqdist'].dtypes != np.float64):
        return (False, 'At least one wrong dtype in the VQDist columns')

    # Check that all vqdist columns sum to 1
    if np.any(dfs['vqdist'].sum(axis=1) != 1):
        return (False, 'At least one VQDist row does not sum to 1')

    if np.any(pd.isna(dfs['data'])):
        return (False, 'NaNs present in data')
    # FIXME check that no state variables are missing. If they are, add
    # columns of zeros and log a warning

    # All checks passed
    return (True,'')

def convert_dates(data_df, start_date=None):
    """Change timestamp index to hours from start date"""
    if start_date is None:
        start_date = data_df.index.get_level_values(0).min()
    dates = data_df.index.get_level_values(0)
    times = (dates - start_date) / np.timedelta64(1, 'h')
    times.name = 'Hours'
    return data_df.set_index(pd.MultiIndex.from_arrays([times,
        data_df.index.get_level_values(1)]))

def write_dat_file(f, dfs, inflow_type, point_st_type, num_statevars=None, hdr_comment=None):
    """Write dfs to a .dat file

    Accepts optional arguments for the number of state variables to write (default is all of them)
    and a header comment"""
    statevars = ALL_STATEVARS if num_statevars is None else ALL_STATEVARS[:num_statevars]
    nodes = len(dfs['nodes'])
    siglayers = len(dfs['vqdist'].columns)
    times = dfs['data'].index.get_level_values(0).drop_duplicates()

    print(f"{inflow_type}  {point_st_type}{' !' + hdr_comment if hdr_comment is not None else ''}", file=f)
    print(len(dfs['nodes']), file=f)
    m = pd.merge(dfs['nodes'], dfs['vqdist'], left_index=True,
        right_index=True, validate='one_to_one', how='left')
    for n,row in m.iterrows():
        print(f"{n:7d}{' ! ' + row['Comment'] if row['Comment'] else ''}", file=f)

    # VQDIST
    nodes_digits = int(np.ceil(np.log10(nodes+0.1)))
    siglay_formatstr = "{:%dd} " % nodes_digits + " ".join(["{:.4f}" for l in dfs['vqdist'].columns])
    for i,(n,row) in enumerate(m.iterrows()):
        print(siglay_formatstr.format(*[i+1] + [row[c] for c in dfs['vqdist'].columns]), file=f)

    # Nqtime
    print(f'{times.size:8d}', file=f)

    # Most values in the original files were formatted with 9.3e, but a few have higher
    # precision so go with this instead
    formatstr = "".join([" {:9.8g}" for l in range(nodes)])
    # Filter out any node data not in the main node list
    filtered_data = dfs['data'].loc[np.isin(dfs['data'].index.get_level_values(1), m.index)]
    for t,group in filtered_data.groupby('Hours'):
        # Ensure the filtered list is sorted the same way as the main nodes
        time_data = group.set_index(group.index.get_level_values(1)
                ).reindex(m.index)
        print(f"     {t:.2f}", file=f)
        for v in statevars:
            print(formatstr.format(*time_data[v].to_numpy()), file=f)

def get_icm_data_only(dfs):
    """Filter out nodes missing from the PNT_WQ file"""
    return {
            'nodes': dfs['nodes'].loc[dfs['nodes']['ICM Source']],
            'vqdist': dfs['vqdist'],
            'data': dfs['data']
    }

def main():
    parser = ArgumentParser(description="Convert a point source discharge spreadsheet into SSM input files")
    parser.add_argument("infile", type=FileType('rb'),
        help="The point sources spreadsheet")
    parser.add_argument("out_base", help="The base of the output filename (_riv.dat and _wq.dat will be appended)")
    parser.add_argument("-s", "--start-date", type=pd.Timestamp,
        help="The zero date for the file. Defaults to the earliest date in the file")
    parser.add_argument("-v", "--verbose", action="store_true", dest="verbose", help="Print progress messages during the conversion")
    parser.add_argument("-c", "--comment", help="An optional comment to include in the first line")
    parser.set_defaults(verbose=False, start_date=None)
    args = parser.parse_args()

    logging.basicConfig(level=logging.INFO if args.verbose else logging.WARNING)

    logger.info("Reading file")
    dfs = read_data(args.infile)

    logger.info("Performing sanity checks")
    res = check_data(dfs, args.start_date)
    if not res[0]:
        logger.error(res[1])
        sys.exit(1)

    logger.info("Adjusting dates/times")
    dfs['data'] = convert_dates(dfs['data'], args.start_date)

    logger.info(f"Writing River file")
    with open(f'{args.out_base}_riv.dat','w') as out_riv_file:
        write_dat_file(out_riv_file, dfs, 'node', 'calculated',
            num_statevars=3, hdr_comment=args.comment)

    logger.info(f"Writing WQ file")
    with open(f'{args.out_base}_wq.dat','w') as out_wq_file:
        write_dat_file(out_wq_file, get_icm_data_only(dfs), 'point', 'calculated',
            hdr_comment=args.comment)
    logger.info("Finished.")

if __name__ == "__main__": main()
